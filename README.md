# Project "Introduction to Data Engineering"

This project was completed as part of the "Introduction to Data Engineering" course. 

During the course, we explored various concepts and technologies, including:

## Interoperability
We learned how to improve data compatibility using standards like SDMX (Statistical Data and Metadata eXchange), Data Cube, and DataSet metadata. These standards and metadata help describe and exchange data across different systems and sources.
## Python and Libraries
We discovered how to use Python for data processing and made use of libraries like csv and os for handling files. We also explored rdflib for working with RDF graphs.
## Loading Data from CSV
We developed skills in loading data from CSV files, processing it, and aggregating it into structured data.
## RDF Graph Creation
 We delved into creating RDF graphs to represent data in RDF (Resource Description Framework) format, which is used for describing data semantically.
## Data Cube Creation
We learned about creating Data Cubes, which are structures for describing multidimensional statistical data.
## Metadata Standards
The project involved the use of standards like DCAT (Data Catalog Vocabulary) for describing data distribution and SKOS (Simple Knowledge Organization System) hierarchy for data classification. This improves the structured representation of metadata.
## SPARQL Queries
We gained expertise in formulating SPARQL queries to extract information from RDF graphs. SPARQL is a query language used for working with RDF data.
